{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "from datetime import datetime\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.functions import from_unixtime\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import year, month, dayofmonth, hour, weekofyear, date_format\n",
    "from pyspark.sql.functions import asc, desc \n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql.functions import sum as Fsum\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('dl.cfg')\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID']=config['AWS']['AWS_ACCESS_KEY_ID']\n",
    "os.environ['AWS_SECRET_ACCESS_KEY']=config['AWS']['AWS_SECRET_ACCESS_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/Users/leopoldwalther/Documents/3_Academic/2_Udacity/4_ND_DataEngineer/4_DataLakesWithSpark/6_ProjectDataLake/s3-data-lake-with-spark-ETL/env380/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.0.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/01/12 15:30:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "\n",
    "def create_spark_session_local():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"local_session\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "# session = create_spark_session()\n",
    "spark = create_spark_session_local()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "input_data = \"../data/\"\n",
    "output_data = \"../output/\"\n",
    "\n",
    "song_data = os.path.join(input_data, \"song_data/*/*/*/*.json\")\n",
    "df_song = spark.read.json(song_data)\n",
    "\n",
    "# extract columns to create songs table\n",
    "songs_table = df_song.select('song_id', 'title', 'artist_id', 'year', 'duration').distinct()\n",
    "    \n",
    "# write songs table to parquet files partitioned by year and artist\n",
    "songs_table.write.partitionBy('year','artist_id') \\\n",
    "    .mode('overwrite') \\\n",
    "    .parquet(output_data + 'songs')\n",
    "\n",
    "# extract columns to create artists table\n",
    "artists_table = df_song.select('artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude').distinct()\n",
    "\n",
    "# write artists table to parquet files\n",
    "artists_table.write.mode('overwrite') \\\n",
    "    .parquet(output_data + 'artists')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/01/12 16:57:57 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# get filepath to log data file\n",
    "log_data = os.path.join(input_data, \"log-data/\")\n",
    "\n",
    "# read log data file\n",
    "df = spark.read.json(log_data)\n",
    "\n",
    "# filter by actions for song plays\n",
    "df = df.where(df['page'] == 'NextSong')\n",
    "\n",
    "# extract columns for users table    \n",
    "users_table = df.select('userId', 'firstName', 'lastName', 'gender', 'level').distinct()\n",
    "\n",
    "# write users table to parquet files\n",
    "users_table.write.mode('overwrite') \\\n",
    "    .parquet(output_data + 'users')\n",
    "\n",
    "# create timestamp column from original timestamp column\n",
    "df = df.withColumn('start_time', from_unixtime((col('ts')/1000).cast('integer')).cast('timestamp'))\n",
    "df = df.withColumn('weekday', date_format(df['start_time'], 'E'))\n",
    "df = df.withColumn('year', year(df['start_time']))\n",
    "df = df.withColumn('month', month(df['start_time']))\n",
    "df = df.withColumn('week', weekofyear(df['start_time']))\n",
    "df = df.withColumn('day', dayofmonth(df['start_time']))\n",
    "df = df.withColumn('hour', hour(df['start_time']))\n",
    "\n",
    "# extract columns to create time table\n",
    "time_table = df.select('start_time', 'weekday', 'year', 'month', 'week', 'day', 'hour').distinct()\n",
    "\n",
    "# write time table to parquet files partitioned by year and month\n",
    "time_table.write.mode('overwrite') \\\n",
    "                .partitionBy('year', 'month') \\\n",
    "                .parquet(path=output_data + 'time')\n",
    "\n",
    "# read in song data to use for songplays table\n",
    "song_data = os.path.join(input_data, \"song_data/*/*/*/*.json\")\n",
    "df_song = spark.read.json(song_data)\n",
    "\n",
    "# extract columns from joined song and log datasets to create songplays table \n",
    "songplays_table = df.join(df_song, (df.song == df_song.title)\\\n",
    "                                       & (df.artist == df_song.artist_name)\\\n",
    "                                       & (df.length == df_song.duration), \"inner\") \\\n",
    "                        .distinct() \\\n",
    "                        .select('start_time', 'userId', 'level', 'song_id', \\\n",
    "                                'artist_id', 'sessionId','location','userAgent', \\\n",
    "                                df['year'].alias('year'), df['month'].alias('month')) \\\n",
    "                        .withColumn(\"songplay_id\", monotonically_increasing_id())\n",
    "\n",
    "# write songplays table to parquet files partitioned by year and month\n",
    "songplays_table.write.mode(\"overwrite\") \\\n",
    "                        .partitionBy('year', 'month')\\\n",
    "                        .parquet(path=output_data + 'songplays')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(artist='Harmonia', auth='Logged In', firstName='Ryan', gender='M', itemInSession=0, lastName='Smith', length=655.77751, level='free', location='San Jose-Sunnyvale-Santa Clara, CA', method='PUT', page='NextSong', registration=1541016707796.0, sessionId=583, song='Sehr kosmisch', status=200, ts=1542241826796, userAgent='\"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Ubuntu Chromium/36.0.1985.125 Chrome/36.0.1985.125 Safari/537.36\"', userId='26')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_song_data(spark, input_data, output_data):\n",
    "    \"\"\" \n",
    "    Extract and transform song data, create songs table and artists tables\n",
    "    \n",
    "        Args:\n",
    "            spark {object}: SparkSession object\n",
    "            input_data {object}: Source S3 bucket\n",
    "            output_data {object}: Target S3 bucket\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "    # get filepath to song data file\n",
    "    song_data = os.path.join(input_data, 'song_data/*/*/*/*.json')\n",
    "    \n",
    "    # read song data file\n",
    "    df = spark.read.json(song_data)\n",
    "\n",
    "    # extract columns to create songs table\n",
    "    songs_table = df_song.select('song_id', 'title', 'artist_id', 'year', 'duration').distinct()\n",
    "    \n",
    "    # write songs table to parquet files partitioned by year and artist\n",
    "    songs_table.write.partitionBy('year','artist_id') \\\n",
    "                        .mode('overwrite') \\\n",
    "                        .parquet(output_data + 'songs')\n",
    "\n",
    "    # extract columns to create artists table\n",
    "    artists_table = df_song.select('artist_id', 'artist_name', 'artist_location', 'artist_latitude', 'artist_longitude').distinct()\n",
    "    \n",
    "    # write artists table to parquet files\n",
    "    artists_table.write.mode('overwrite') \\\n",
    "                        .parquet(output_data + 'artists')\n",
    "\n",
    "\n",
    "def process_log_data(spark, input_data, output_data):\n",
    "    \"\"\" Process log data, create users table, time table and songplays table\n",
    "\n",
    "        Args:\n",
    "            spark {object}: SparkSession object\n",
    "            input_data {object}: Source S3 bucket\n",
    "            output_data {object}: Target S3 bucket\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "    \"\"\"\n",
    "\n",
    "    # get filepath to log data file\n",
    "    log_data = os.path.join(input_data, \"log-data/\")\n",
    "\n",
    "    # read log data file\n",
    "    df = spark.read.json(log_data)\n",
    "\n",
    "    # filter by actions for song plays\n",
    "    df = df.where(df['page'] == 'NextSong')\n",
    "\n",
    "    # extract columns for users table    \n",
    "    users_table = df.select('userId', 'firstName', 'lastName', 'gender', 'level').distinct()\n",
    "\n",
    "    # write users table to parquet files\n",
    "    users_table.write.mode('overwrite') \\\n",
    "        .parquet(output_data + 'users')\n",
    "\n",
    "    # create timestamp column from original timestamp column\n",
    "    df = df.withColumn('start_time', from_unixtime((col('ts')/1000).cast('integer')).cast('timestamp'))\n",
    "    df = df.withColumn('weekday', date_format(df['start_time'], 'E'))\n",
    "    df = df.withColumn('year', year(df['start_time']))\n",
    "    df = df.withColumn('month', month(df['start_time']))\n",
    "    df = df.withColumn('week', weekofyear(df['start_time']))\n",
    "    df = df.withColumn('day', dayofmonth(df['start_time']))\n",
    "    df = df.withColumn('hour', hour(df['start_time']))\n",
    "\n",
    "    # extract columns to create time table\n",
    "    time_table = df.select('start_time', 'weekday', 'year', 'month', 'week', 'day', 'hour').distinct()\n",
    "\n",
    "    # write time table to parquet files partitioned by year and month\n",
    "    time_table.write.mode('overwrite') \\\n",
    "                    .partitionBy('year', 'month') \\\n",
    "                    .parquet(path=output_data + 'time')\n",
    "\n",
    "    # read in song data to use for songplays table\n",
    "    song_data = os.path.join(input_data, \"song_data/*/*/*/*.json\")\n",
    "    df_song = spark.read.json(song_data)\n",
    "\n",
    "    # extract columns from joined song and log datasets to create songplays table \n",
    "    songplays_table = df.join(df_song, (df.song == df_song.title)\\\n",
    "                                        & (df.artist == df_song.artist_name)\\\n",
    "                                        & (df.length == df_song.duration), \"inner\") \\\n",
    "                            .distinct() \\\n",
    "                            .select('start_time', 'userId', 'level', 'song_id', \\\n",
    "                                    'artist_id', 'sessionId','location','userAgent', \\\n",
    "                                    df['year'].alias('year'), df['month'].alias('month')) \\\n",
    "                            .withColumn(\"songplay_id\", monotonically_increasing_id())\n",
    "\n",
    "    # write songplays table to parquet files partitioned by year and month\n",
    "    songplays_table.write.mode(\"overwrite\") \\\n",
    "                            .partitionBy('year', 'month')\\\n",
    "                            .parquet(path=output_data + 'songplays')\n",
    "\n",
    "\n",
    "def main():\n",
    "    spark = create_spark_session()\n",
    "    input_data = \"s3a://udacity-dend/\"\n",
    "    output_data = \"\"\n",
    "    \n",
    "    process_song_data(spark, input_data, output_data)    \n",
    "    process_log_data(spark, input_data, output_data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d050fea425fc3a0c44864ec75a53d9c3f70290418f36a4682b185d6d12e2c70f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('env380': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
